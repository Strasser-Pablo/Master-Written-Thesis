\chapter{Fixed}
\minitoc
\section{Introduction}

We will in this chapter be interested in the case of fixed domain Navier-Stokes with grid discretisation.
This case is easier than the variable domain case and serve as basis to more complicated case.

\section{Overview}

We will in this chapter present two way to solve Navier-Stokes equation.
We will show that this two method are equivalent if a Runge-Kutta method is used to solve.

The two method are presented at next section.
\subsection{Analytical expression for pressure}
\label{fixed:analytical}

We use the work of section \ref{analytical:fixe_eulerian} that Navier-Stokes equation is analytically equivalent to:
\begin{equation}
  \partial_t \vect{v}(\vect{x} ,t)=(\eye+P)f(\vect{v}(\vect{x},t))
\end{equation}

$\eye+P$ project a none divergence free vector to a divergence free vector.
The solution $\vect{v}$ is a divergence free vector. Because the integral of a divergence free vector is again a divergence free vector.

But a priori the numerical solution is not necessary divergence free.

\subsection{Projection of speed}
\label{fixed:proj}

We can project every speed in a divergence free space:
\begin{subequations}
\begin{align}
  \partial_t \vect{\tilde{v}}(\vect{x} ,t)&=f(\vect{v}(\vect{x},t))\\
  \vect{v}(\vect{x},t)&=(\eye+P)\vect{\tilde{v}}
\end{align}
\end{subequations}

We have the certitude to be always divergence free by construction.

\section{Runge-Kutta}

A Runge-Kutta method is a method to solve the ODE (I drop the $\vect{x}$ dependence, but it's understood that this equation is for a vector formed in concatenating every speed):
\begin{equation}
\partial_t \vect{v}(t)=f(\vect{v})
\end{equation}

The solution at time $n+1$ is given from the solution at time $n$ with $\Delta t$ the time step and $a$, $b$ parameter of the method.
\begin{subequations}
\begin{align}
	\vect{v}_{n+1}&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}k_{i}\\
	k_{j}&=\Delta t f(\vect{v}_{n}+\sum_{i=1}^{j-1}a_{j,i}k_{i})
\end{align}
\end{subequations}

\section{Runge-Kutta based scheme}

We now integrate method \label{fixed:analytical} and \label{fixed:proj} with a Runge-Kutta method.

Scheme for projection at every evaluation of speed:
\begin{subequations}
\begin{align}
\vect{\tilde{v}}_{n+1}&=\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}+P\left(\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}\right)\\
\tilde{k}_{i}&=\Delta t f\left(\vect{\tilde{v}}_{n}+\sum_{j=1}^{i-1}a_{ij}\tilde{k}_{j}+P\left(\vect{\tilde{v}}_{n}+\sum_{j=1}^{i-1}a_{ij}\tilde{k}_{j}\right)\right)
\end{align}
\end{subequations}

We project at every estimation of speed. Making by construction every speed divergence free.

Scheme for analytical expression for pressure:
\begin{subequations}
\begin{align}
	\vect{v}_{n+1}&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}k_{i}\\
	k_{i}&=\Delta t f\left(\vect{v}_{n}+\sum_{j=1}^{i-1}a_{ij}k_{j}\right)+\Delta t P\left(f\left(\vect{v}_{n}+\sum_{j=1}^{i-1}a_{ij}k_{j}\right)\right)
\end{align}
\end{subequations}

The following theorem show that the two scheme are the same.

\begin{thm}
If $\vect{v}_{n}=\vect{\tilde{v}}_{n}$ and $\vect{v}_n$ is divergence free then $\vect{v}_{n+1}=\vect{\tilde{v}}_{n+1}$ 
\end{thm}
\begin{proof}
We begin by prove the following lemma.
\begin{lem}
\begin{equation}
  k_{i}=\tilde{k}_{i}+P(\tilde{k}_{i})
\end{equation}
\end{lem}
\begin{proof}
By recurrence on $i$.
We begin with $i=1$:
\begin{equation}
  k_{1}=\Delta tf\left(\vect{v}_n\right)+\Delta tP\left(f\left(\vect{v}_n\right)\right)
\end{equation}
\begin{equation}
\tilde{k}_{1}=\Delta tf\left(\vect{v}_n\right)
\end{equation}
\begin{equation}
  k_{1}=\Delta t f\left(\vect{v}_n\right)+\Delta tP\left(f\left(\vect{v}_n\right)\right)=\tilde{k}_1+P\left(\tilde{k}_1\right)
\end{equation}

Assuming true for smaller $i$:
\begin{align*}
  k_{i}&=\Delta tf\left(\vect{v}_n+\sum_{j=1}^{n}a_{ij}k_{j}\right)+\Delta tP\left(f\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}k_{ij}\right)\right)\\
  &=\Delta tf\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}\left(\tilde{k}_{j}+P\left(\tilde{k}_{j}\right)\right)\right)+\Delta tP\left(f\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}\left(\tilde{k}_{j}+P\left(\tilde{k}_{j}\right)\right)\right)\right)\\
  \tilde{k}_{i}&=\Delta tf\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}\tilde{k}_{j}+P\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}\tilde{k}_{j}\right)\right)\\
  &=\Delta tf\left(\vect{v}_n+\sum_{j=1}^{i-1}a_{ij}\left(\tilde{k}_{j}+P\left(\tilde{k}_{j}\right)\right)\right)\\
  k_{i}&=\tilde{k}_{i}+P\left(\tilde{k}_{i}\right)
\end{align*}
\end{proof}

We now write the expression for $\vect{v}_{n+1}$ and $\vect{\tilde{v}}_{n+1}$ using the lemma.

\begin{align*}
\vect{v}_{n+1}&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}k_{i}\\
&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}\left(\tilde{k}_{i}+P\left(\tilde{k}_{i}\right)\right)\\
\vect{\tilde{v}}_{n+1}&=\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}+P\left(\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}\right)\\
&=\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\left(\tilde{k}_{i}+P\left(\tilde{k}_{i}\right)\right)\\
\intertext{Using that $\vect{v}_{n}=\tilde{\vect{v}_{n}}$}
\vect{v}_{n+1}&=\vect{\tilde{v}}_{n+1}
\end{align*}

\end{proof}

\begin{thm}
For the implicit Runge-Kutta case, where the sum to $i-i$ go now to $s$.
If $\vect{v}_{n}=\vect{\tilde{v}}_{n}$ and $\vect{v}_n$ is divergence free then $\vect{v}_{n+1}=\vect{\tilde{v}}_{n+1}$ 
\end{thm}
\begin{proof}
We begin by prove the following lemma.
\begin{lem}
After relabeling noting $k_{i}^{j}$ the $j$ solution of $k_{i}$.
We have for every $j$.
\begin{equation}
  k_{i}=\tilde{k}_{i}^{j}+P(\tilde{k}_{i}^{j})
\end{equation}
This doesn't say that we have existence and unicity of the solution only that if we have a set of solution for $k$ we have a bijection
to solution of $\tilde{k}$
\end{lem}
\begin{proof}
Because the sum goes to $s$ and not $i-1$ we cannot use recurrence.
We now will use vector notation for $k$.

We construct $k$ and $\tilde{k}$ vector with the vector $k_{i}$ and $\tilde{k}_{i}$
\begin{align}
k&=\begin{pmatrix}
    k_{1}\\
    \vdots\\
    k_{s}\\
  \end{pmatrix}\\
\tilde{k}&=\begin{pmatrix}
    \tilde{k}_{1}\\
    \vdots\\
    \tilde{k}_{s}\\
  \end{pmatrix}
\end{align}

We form $\hat{P}$ and $\hat{f}$ from a Kronecker product:
\begin{align}
\hat{P}&=\eye_{s}\kron P=\begin{pmatrix}P	&\ldots	&0\\
			\vdots &\ddots 	&\vdots\\
			0	&0	&P\\
	\end{pmatrix}\\
\hat{f}&=\eye_{s}\kron f\begin{pmatrix}f	&\ldots	&0\\
			\vdots &\ddots 	&\vdots\\
			0	&0	&f\\
	\end{pmatrix}
\end{align}

We now define $\hat{v}_n$:
\begin{equation}
\hat{v}_{n}=\begin{pmatrix}
	      v_{n}\\
	      \vdots\\
	      v_{n}
	      \end{pmatrix}
\end{equation}

We now define $\hat{A}$ with the following Kronecker product:
\begin{equation}
\hat{A}=\begin{pmatrix}
    a_{11}	&\ldots	&a_{1s}\\
    \vdots	&\ddots	&\vdots\\
    a_{s1}	&\ldots	&a_{ss}\\
  \end{pmatrix} \kron \eye=A \kron \eye
\end{equation}

$\hat{A}$ and $\hat{P}$ are commutative, because of the mixed-product property.
\begin{align}
\hat{A}\hat{P}&=\left(A\kron \eye \right)
  \left(\eye_{s}\kron P\right)=
	A\kron P\\
    %
    \hat{P}\hat{A}&=
  \left(\eye_{s}\kron P\right)
	\left(A \kron \eye \right)=
	A\kron P
\end{align}

\begin{align}
k&=(1+\hat{P})\Delta t\hat{f}(\hat{v}_{n}+\hat{A}k)\\
\tilde{k}&=\Delta t \hat{f}((1+\hat{P})(\hat{v}_{n}+\hat{A}k))
\end{align}

We begin to define 2 functions:
\begin{align}
k&=g(k)\\
\tilde{k}&=\tilde{g}(\tilde{k})\\
g(x)&=(1+\hat{P})\Delta t \hat{f}(\hat{v}_{n}+\hat{A}x)\\
\tilde{g}(x)&=\Delta t \hat{f}((1+\hat{P})(\hat{v}_{n}+\hat{A}x))
\end{align}

$k$ and $\tilde{k}$ are fixed point of function $g$ and $\tilde{g}$.

We now calculate:
\begin{align}
(1+\hat{P})\tilde{g}(x)&=(1+\hat{P})\Delta t \hat{f}((1+\hat{P})(\hat{v}_{n}+\hat{A}x))=(1+\hat{P})\Delta t \hat{f}(\hat{v}_{n}+\hat{A} (1+\hat{P})x)\\
&=g((1+\hat{P})x)
\end{align}

We now substitute $g$ in $\tilde{g}$
\begin{equation}
(1+\hat{P})\tilde{k}=g((1+\hat{P})\tilde{k})
\end{equation}

$(1+\hat{P})\tilde{k}$ is a fixed point of $g$.
But fixed point of $g$ is $k$.

This directly give the result with $k^{j}$ the $j$ solution:
\begin{equation}
k^{j}=(1+\hat{P})\tilde{k}^{j}
\end{equation}
This is exactly the vector version of what we wanted to prove.
\end{proof}

The rest is exactly the same than in the first case.

To not overload the proof with notation, we drop the $j$ label to indicate witch solution we take.
We now write the expression for $\vect{v}_{n+1}$ and $\vect{\tilde{v}}_{n+1}$ using the lemma.
\begin{align*}
\vect{v}_{n+1}&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}k_{i}\\
&=\vect{v}_{n}+\sum_{i=1}^{s}b_{i}\left(\tilde{k}_{i}+P\left(\tilde{k}_{i}\right)\right)\\
\vect{\tilde{v}}_{n+1}&=\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}+P\left(\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\tilde{k}_{i}\right)\\
&=\vect{\tilde{v}}_{n}+\sum_{i=1}^{s}b_{i}\left(\tilde{k}_{i}+P\left(\tilde{k}_{i}\right)\right)\\
\intertext{Using that $\vect{v}_{n}=\tilde{\vect{v}_{n}}$}
\vect{v}_{n+1}&=\vect{\tilde{v}}_{n+1}
\end{align*}

\end{proof}

\begin{cor}
Solving Navier-Stokes equation with the analytical good choice of pressure.
Is exactly divergence free at every approximation of speed.
\end{cor}
\begin{proof}
  The expression of $v$ is the solution of the Navier-Stokes equation with the good choice of pressure.
  $\tilde{v}$ is the solution of Runge-Kutta equation without pressure witch is projected at every speed estimation.
  This is divergence free.
  $v$ and $\tilde{v}$ are the same. With end the proof.
\end{proof}

\begin{cor}
  Using the expression for $v$ or $\tilde{v}$ we are $k$ order precise in time. Where $k$ is the order of the Runge-Kutta method.
\end{cor}
\begin{proof}
  $v$ and $\tilde{v}$ are the same.
  $v$ consist only of a Runge-Kutta method with a good chosen function.
  So the order in time of $v$ and $\tilde{v}$ is the same that the method order.
\end{proof}

\section{Spatial discretization}

\subsection{Unstaggered grid}

On an unstaggered grid we put the variable like figure \ref{fixed:unstaggered}.

\begin{figure}
\directlua{dofile('fixed/unstaggered.lua')}
\caption{Position of variable for unstaggered grid}
\label{fixed:unstaggered}
\end{figure}

The problem with unstaggered grid, is that obvious center difference discretization lead to wrong divergence free vector.

We define the derivative of a scalar by:
\begin{equation}
  \partial_x a_i=\frac{a_{i+1}-a{i-1}}{\Delta x}
\end{equation}

The divergence discretisation is then given by (notation for 2d, but for 3d it's the same):
\begin{equation}
  \vect{\nabla} \cdot \vect{v}_{i,j}=\frac{{v_{x}}_{i+1,j}-{v_{x}}_{i-1,j}}{\Delta x}+\frac{{v_{y}}_{i+1,j}-{v_{y}}_{i-1,j}}{\Delta y}
\end{equation}

The problem can be see in figure \ref{fixed:unstaggered_div} is that the divergence at red point only use value at blue point and not the point at center (red point).
This allow to have none physical solution that are divergence free.

\begin{figure}
\directlua{dofile('fixed/unstaggered_div.lua')}
\caption{Value used to calculate the divergence at red point are in blue}
\label{fixed:unstaggered_div}
\end{figure}

In figure \ref{fixed:unstaggered_div2} we have solution that is numerically divergence free. But this solution is not continuous and we expect that is not divergence free.

\begin{figure}
\directlua{dofile('fixed/unstaggered_div2.lua')}
\caption{Speed that are numerically divergence free but should'nt}
\label{fixed:unstaggered_div2}
\end{figure}

The problem with this kind of solution, is that it's very stable.
The projection operator $P$ will be 0, and don't correct this kind of speed. We cannot correct it in general for every choice of $f$.

We can recall one of the formulation of the so call Murphy's law, ``If anything can go wrong, it will.''

We can correct this problem with using a forward or backward expression. But this kind of expression is only first order accurate.

Another solution is to use another grid.

\subsection{Staggered grid}

On a staggered grid we put the variable in the position shown if figure \ref{fixed:staggere}.

\begin{figure}
\directlua{dofile('fixed/staggered.lua')}
\caption{Position of variable on a staggered grid}
\label{fixed:staggered}
\end{figure}

This can seem to be strange to have different component of speed at different place. But it make discretization more easy.

To label variable, we label by cell and then took the lower,left component (cf. figure \ref{fixed:staggered_label}).

\begin{figure}
\directlua{dofile('fixed/staggered_label.lua')}
\caption{Labeling on staggered grid}
\label{fixed:staggered_label}
\end{figure}

We now define the divergence and the gradient:
\begin{align}
  \vect{\nabla}p_{ij}&=\begin{pmatrix}
    \frac{p_{i,j}-p_{i-1,j}}{\Delta x}\\
    \frac{p_{i,j}-p_{i,j-1}}{\Delta y}\\
                      \end{pmatrix}\\
  \vect{\nabla}\cdot \vect{v}_{i,j}&=\frac{{v_{x}}_{i+1,j}-{v_{x}}_{i,j}}{\Delta x}+\frac{{v_{y}}_{i,j+1}-{v_{y}}_{i,j}}{\Delta y}
\end{align}

This two expression are geometrically very clear.

In figure \ref{fixed:staggered_gradient}, the gradient at red speed component use component at blue point.
Contrary at case with center difference we cannot create a none constant vector with 0 gradient.

\begin{figure}
\directlua{dofile('fixed/staggered_gradient.lua')}
\caption{Point used to calculate the red gradient component are in blue}
\label{fixed:staggered_gradient}
\end{figure}

The divergence is given by the sum of inflow around a cell.
In figure \ref{fixed:staggered_divergence}, the divergence at red point use component at blue point.

\begin{figure}
\directlua{dofile('fixed/staggered_divergence.lua')}
\caption{Point used to calculate red divergence are in blue}
\label{fixed:staggred_divergence}
\end{figure}

We cannot have the same situation than without staggered grid, where a unphysical vector is divergence free.
